# -*- coding: utf-8 -*-
"""hypothetical_document_embedding_rag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g-ba-3VRx7m_wAkwhFMOM_8FiJH3ApKI

<a href="https://github.com/genieincodebottle/generative-ai/blob/main/genai-usecases/advance-rag/hypothetical-document-embedding-rag/hypothetical-document-embedding-rag.ipynb" target="_parent"><img src="https://img.shields.io/badge/Open in GitHub-181717?style=flat&logo=github&logoColor=white" alt="Open In GitHub"></a>

#	üìÑ What is HyDE?

Hypothetical Document Embeddings (HyDE) is an advance RAG technique that generates synthetic document embeddings based on the query, rather than relying solely on existing document embeddings. This approach creates a "hypothetical" perfect document that would ideally answer the query, and then uses this embedding to retrieve the most similar actual documents from the database. HyDE aims to bridge the gap between query intent and document content, especially for complex or nuanced queries.

# üîß HyDE-RAG Implementation:

1. **Hypothetical Document Generation:** We use the LLM to generate a hypothetical passage that would answer the given query.
2. **Retrieval using Hypothetical Document:** We use the generated hypothetical document to retrieve relevant passages from our vector store.
3. **Final Response Generation:** Using the retrieved context, we generate a final response to the original query.

# ‚öôÔ∏è Setup

1. **[LLM](https://deepmind.google/technologies/gemini/pro/):** Google's free gemini-pro api endpoint ([Google's API Key](https://console.cloud.google.com/apis/credentials))
2. **[Vector Store](https://www.pinecone.io/learn/vector-database/):** [ChromaDB](https://www.trychroma.com/)
3. **[Embedding Model](https://qdrant.tech/articles/what-are-embeddings/):** [nomic-embed-text-v1.5](https://www.nomic.ai/blog/posts/nomic-embed-text-v1)
4. **[LLM Framework](https://python.langchain.com/v0.2/docs/introduction/):** LangChain
5. **[Huggingface API Key](https://huggingface.co/settings/tokens)**

# Install required libraries
"""

# !pip install -q -U \
#      Sentence-transformers==3.0.1 \
#      langchain==0.2.11 \
#      langchain-google-genai==1.0.7 \
#      langchain-chroma==0.1.2 \
#      langchain-community==0.2.10 \
#      langchain-huggingface==0.0.3 \
#      einops==0.8.0

"""# Import related libraries related to Langchain, HuggingfaceEmbedding"""

from langchain_google_genai import (
    ChatGoogleGenerativeAI,
    HarmBlockThreshold,
    HarmCategory,
)
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.prompts import ChatPromptTemplate
from langchain.document_loaders import WebBaseLoader

import os
import getpass

"""# Provide Google API Key. You can create Google API key at following link

[Google Gemini-Pro API Creation Link](https://console.cloud.google.com/apis/credentials)

[YouTube Video](https://www.youtube.com/watch?v=ZHX7zxvDfoc)


"""

#os.environ["GOOGLE_API_KEY"] = getpass.getpass()

"""# Provide Huggingface API Key. You can create Huggingface API key at following link

[Huggingface API Creation Link](https://huggingface.co/settings/tokens)



"""

os.environ["HF_TOKEN"] = getpass.getpass()

"""# Step 1: Load and preprocess data code"""

def load_and_process_data(url):
    # Load data from web
    loader = WebBaseLoader(url)
    data = loader.load()

    # Split text into chunks (Experiment with Chunk Size and Chunk Overlap to get optimal chunking)
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = text_splitter.split_documents(data)

    return chunks

"""# Step 2: Create vector store code"""

def create_vector_store(chunks):
    embeddings = HuggingFaceEmbeddings(model_name="nomic-ai/nomic-embed-text-v1.5", model_kwargs = {'trust_remote_code': True})
    vectorstore = Chroma.from_documents(chunks, embeddings)
    return vectorstore

"""# Step 3: HyDE-RAG related code

1. **Hypothetical Document Generation:** We use the LLM to generate a hypothetical passage that would answer the given query.
2. **Retrieval using Hypothetical Document:** We use the generated hypothetical document to retrieve relevant passages from our vector store.
3. **Final Response Generation:** Using the retrieved context, we generate a final response to the original query.
"""

def hyde_rag(query, vectorstore, llm):
    # Generate hypothetical document
    hyde_prompt = ChatPromptTemplate.from_template(
        "Given the following question, generate a hypothetical passage that would answer this question:\nQuestion: {query}\nHypothetical Passage:"
    )
    hyde_chain = hyde_prompt | llm
    hypothetical_doc = hyde_chain.invoke({"query": query})

    # Retrieve relevant documents using the hypothetical document
    retrieved_docs = vectorstore.similarity_search(hypothetical_doc.content, k=3)
    context = "\n".join([doc.page_content for doc in retrieved_docs])

    # Generate final response
    final_prompt = ChatPromptTemplate.from_template(
        "Based on the following context, please answer the question:\nContext: {context}\nQuestion: {query}\nAnswer:"
    )
    final_chain = final_prompt | llm
    final_response = final_chain.invoke({"context": context, "query": query})

    return {
        "hypothetical_document": hypothetical_doc.content,
        "retrieved_context": context,
        "final_answer": final_response.content
    }

"""# Step 4: Create chunk of web data to Chroma Vector Store"""

# Initialize the gemini-pro language model with specified settings (Change temeprature  and other parameters as per your requirement)
llm = ChatGoogleGenerativeAI(model="gemini-1.5-pro", google_api_key='AIzaSyCW79FJ7MHFhrkpYNYsqImgGtcJw56Ruhg', temperature=0.3, safety_settings={
          HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,
        },)

# Load and process data
url = "https://en.wikipedia.org/wiki/Artificial_intelligence"
chunks = load_and_process_data(url)

# Create vector store
vectorstore = create_vector_store(chunks)

"""# Step 5: Run HyDE-RAG

This implementation shows the key parts of HyDE-RAG:

1. Generation of a hypothetical document based on the query
2. Using the hypothetical document for retrieval instead of the original query
3. Final response generation using the retrieved context
"""

# Example query
query = "What are the ethical considerations in AI development?"
result = hyde_rag(query, vectorstore, llm)

print("Hypothetical Document:")
print(result["hypothetical_document"])
print("\nRetrieved Context:")
print(result["retrieved_context"])
print("\nFinal Answer:")
print(result["final_answer"])